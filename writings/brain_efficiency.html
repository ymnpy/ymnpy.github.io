<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@400&display=swap" rel="stylesheet">
    
    <title>efficiency of human brain</title>
    <link rel="shortcut icon" type="image/png" href="../images/favicon1.png">
    <link rel="stylesheet" href="../styles/style_others.css">

</head>
<body>
    <br><span>November 11, 2025</span>
    <div class="overlay"></div>
    <h1>efficiency of human brain<</h1>
    <div class="content">
        <p>
        I have been doing some machine learning stuff for almost like 2 years if I am not mistaken. This subject gave me lots of insights about ourselves, gave ideas about how we learn, memorize, how we build up experiences etc. But I want to share one of them which amazed me lately, as the rest of them.

        I have built a machine learning model last week. It is a regression model, a cute neural network, vanilla one, one they call <i>multi player perceptrop (MLP)</i>. This model has 3 hidden layers and bunch of neurons. For two regression predictions, I have modeled two of them. This model takes +10 parameters and give 1 output, a continuous value. Once I have pickled it, or joblib-ed it (which I have actually used this one instead of pickle) it took under 1 MB of size and its actually doing something really cool like solving or predicting a non-linear function. Also it is super duper fast. So in short, it takes little space and fast, so it is pretty efficient. Then it made me think about our brain. I remember hearing about how we don't actually memorize everything we just build neural nets inside of our brain before (probably from a podcast, and probably from <i>geohotz</i>). This thought came to my mind again. Think about this: instead of remembering 100 differen numbers you just build model that gives those 100 number once the model is fired-up. For instance: you don't memorize proton numbers of each element you just buil a neural network for it and ask the proton number, it fires up, give you the exact number. So you do not actually store those number in your memory, you just store the network. Maybe this is not the best example but hopefully you got the idea. In my case it was similar to this. This model is able to predict +10 parameters function, and gives output numbers of lets say 0-20, in a continuos manner so there are (I guess) infinite amount of numbers that can be output. However, this model only costs you, let's say, 300 KB. It is amazing. I assume there is something similar going on in our brain gives information when we needed by having appropriate networks for it instead of memorizing every possible answer. Maybe both of them are going on. Maybe you become kind of expert once you build up the network instead of memorizing the answers. Of course this takes many trainings, in other words many experiences. But, we are also more efficient in this case as well but thats another topic.

        In short our brain is amazing. It is possible that it does what it does with relatively small amount of resource because of its efficiency, because it builds networks so they can be used later on for the required information.
        </p>

    </div>
    <a class="go_back", href="../index.html">Go back</a>
</body>


</html>